# AdaboostOnMNIST
This is an implementation of the Adaboost algorithm from scratch using two different weak learners: Decision Tree Classifiers and Gradient Boost Classifiers. Adaboost run on MNIST to tell odd vs even numbers. Tested against scikit Learn model for adaboost and got a better score. Lowest training error was %1.8 with gradient boosting on 7 iterations.
